# Tool learning

## Survey

- [Arxiv'24] Tool Learning with Foundation Models [[paper](https://arxiv.org/abs/2304.08354)] [[code](https://github.com/thunlp/ToolLearningPapers)] [[software](https://github.com/OpenBMB/BMTools)]  **OpenBMB**

- [Arxiv'24] Tool Learning with Large Language Models: A Survey [[paper](https://arxiv.org/abs/2405.17935)] [[code](https://github.com/quchangle1/LLM-Tool-Survey)]

- Awesome-Tool-Learning
 [[Github1](https://github.com/luban-agi/Awesome-Tool-Learning)]  [[Github2](https://github.com/zorazrw/awesome-tool-llm)]

---

## NLP

- [TMLR'23] Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks [[paper](https://arxiv.org/abs/2211.12588)] [[code](https://github.com/wenhuchen/program-of-thoughts)]

- [NIPS'23] Toolformer: Language models can teach themselves to use tools [[paper](https://arxiv.org/abs/2302.04761)]

- [ICML'23] PAL: Program-aided Language Models [[paper](https://arxiv.org/abs/2211.10435)]

---

## Multi-modal

- [Arxiv'23] MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action [[paper](https://arxiv.org/abs/2303.11381)] [[code](https://github.com/microsoft/MM-REACT)] **Microsoft**

- [Arxiv'23] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models [[paper](https://arxiv.org/abs/2303.04671)] [[code](https://github.com/chenfei-wu/TaskMatrix)] **Microsoft**

- [CVPR'23] Visual Programming: Compositional visual reasoning without training [[paper](https://arxiv.org/abs/2211.11559)] [[code](https://github.com/allenai/visprog)] **best paper**

    - [CVPR'24] SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World [[paper](https://arxiv.org/abs/2312.02976)] [[code](https://github.com/allenai/spoc-robot-training)] [[project](https://spoc-robot.github.io/)] **机器人在现实世界中的导航和操控**

    - [CVPRW'24] m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks [[paper](https://arxiv.org/abs/2403.11085)] [[code](https://github.com/RAIVNLab/mnms)] [[project](https://mnms-project.github.io/)] **包含4000+多步骤多模态任务的基准数据集**

    - [Arxiv'24] CodeNav: Beyond tool-use to using real-world codebases with LLM agents [[paper](https://arxiv.org/abs/2406.12276v1)] [[code](https://github.com/allenai/codenav)] [[project](https://codenav.allenai.org/)] **导航并利用以前未见过的代码库（Visprog进阶）**

- [ICCV'23] ViperGPT: Visual Inference via Python Execution for Reasoning [[paper](https://arxiv.org/abs/2303.08128)] [[code](https://github.com/cvlab-columbia/viper)] [[project](https://viper.cs.columbia.edu/)]

- [Arxiv'23] ControlLLM: Augment Language Models with Tools by Searching on Graphs [[paper](https://arxiv.org/abs/2310.17796)] [[code](https://github.com/opengvlab/controlllm)] **Shanghai AILab**

- [ICLR'24] ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world API [[paper](https://arxiv.org/abs/2307.16789)] [[code](https://github.com/openbmb/toolbench)] **OpenBMB**

- [NIPS'23] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face [[paper](https://arxiv.org/abs/2303.17580)] [[code](https://github.com/microsoft/JARVIS)] **Microsoft**

    - [ICLR'24] TaskBench: Benchmarking Large Language Models for Task Automation [[paper](https://arxiv.org/abs/2311.18760)] [[code](https://github.com/microsoft/JARVIS/tree/main/taskbench)]

    - [ICLRW'24] EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction [[paper](https://arxiv.org/abs/2401.06201)] [[code](https://github.com/microsoft/JARVIS/tree/main/easytool)]

- [NIPS'24] Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models [[paper](https://arxiv.org/abs/2304.09842)] [[code](https://github.com/lupantech/chameleon-llm)]  **Microsoft**

---

## Others

- [ICCV'17] Inferring and Executing Programs for Visual Reasoning [[paper](https://arxiv.org/abs/1705.03633)] [[code](https://github.com/facebookresearch/clevr-iep)] **Meta (Feifei Li)**

- [NIPS'22] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [[paper](https://arxiv.org/abs/2201.11903)] **Google Brain**