# LargeModel

## LLAVA (Offical)

- [NIPS'23] Visual Instruction Tuning [[paper](https://arxiv.org/abs/2304.08485)] [[code](https://github.com/haotian-liu/LLaVA)] [[demo](https://llava.hliu.cc/)]
    
    - [2023/10/5] LLaVA-1.5 [[Tech report](https://arxiv.org/abs/2310.03744)]

    - [2023/10/26] LLaVA-1.5-LoRA [[Doc finetune](https://github.com/haotian-liu/LLaVA/blob/main/docs/Finetune_Custom_Data.md)]

    - [2024/1/30] LLaVA-NeXT [[Blog post](https://llava-vl.github.io/blog/2024-01-30-llava-next/)]

- [NIPS'23] LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day [[paper](https://arxiv.org/abs/2306.00890)] [[code](https://github.com/microsoft/LLaVA-Med)]

- [2023/9/26] Aligning Large Multimodal Models with Factually Augmented RLHF [[paper](https://arxiv.org/abs/2309.14525)] [[code](https://github.com/llava-rlhf/LLaVA-RLHF)]

- [2023/11/2] LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing [[paper](https://arxiv.org/abs/2311.00571)] [[code](https://github.com/LLaVA-VL/LLaVA-Interactive-Demo)]

- [2023/11/10] LLaVA-Plus: Large Language and Vision Assistants that Plug and Learn to Use Skills [[paper](https://arxiv.org/abs/2311.05437)] [[code](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase)]

- [2023/12/6] LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models [[paper](https://arxiv.org/abs/2312.02949)] [[code](https://github.com/UX-Decoder/LLaVA-Grounding)]

## LLAVA-based  (Unoffical)

- [CVPRW'24] Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs [[paper](https://arxiv.org/abs/2404.15406)]

- [Arxiv'24] PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning [[paper](https://arxiv.org/abs/2404.16994)] [[code](https://github.com/magic-research/PLLaVA)] [[blog](https://mp.weixin.qq.com/s/_3vlEMcqL3fOp8fRHtMvCQ)]

- [Arxiv'24] LLaVA-Phi & Mipha: Towards Multimodal Small Language Models [[paper](https://arxiv.org/abs/2401.02330)] [[code](https://github.com/zhuyiche/llava-phi)]

- [Arxiv'24] ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models [[paper](https://arxiv.org/abs/2405.15738)] [[code](https://github.com/alibaba/conv-llava)] [[blog](https://mp.weixin.qq.com/s/_MWpuGLAj3D8D1Ijdf197g?poc_token=HLqTamaj2XbCc4vt7vWI3iEjBbdTBh_LtKdR-l64)] **Alibaba**

- [Arxiv'24] TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models [[paper](https://arxiv.org/abs/2405.11788)] [[code](https://github.com/TinyLLaVA/TinyLLaVA_Factory)] [[HF](https://huggingface.co/tinyllava)] [[blog](https://mp.weixin.qq.com/s/rQM9pXn4s8KZcXy7DW63kw)] **支持定制、训练、评估多模态大模型**

- [Arxiv'24] LLaVA-CoT: Let Vision Language Models Reason Step-by-Step [[code](https://wisemodel.cn/codes/KevinTHU/LLaVA-CoT)]
    - [Arxiv'24] LLaVA-o1: Let Vision Language Models Reason Step-by-Step [[code](https://github.com/PKU-YuanGroup/LLaVA-o1)] **Previous Version**

## MLLM

- [ICLR'24] AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection [[paper](https://arxiv.org/abs/2310.18961)] [[code](https://github.com/zqhang/AnomalyCLIP)]

- [Arxiv'24] WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs [[paper](https://arxiv.org/abs/2403.07944)]

- [Arxiv'24] Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want [[paper](https://arxiv.org/abs/2403.20271)] [[code](https://github.com/AFeng-x/Draw-and-Understand)]

- [Arxiv'24] PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model [[paper](https://arxiv.org/abs/2403.14598)] [[code](https://github.com/zamling/PSALM)] [[HugFace](https://huggingface.co/EnmingZhang/PSALM)]

- [ECCV'24] VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks [[paper](https://arxiv.org/abs/2403.00522)] [[code](https://github.com/meituan-automl/visionllama)] [[blog](https://mp.weixin.qq.com/s/xPrmghVVEzl5BPx3IPpREw)] 