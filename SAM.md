# Segment Anything Model

## Survey
- Awesome-Segment-Anything [[Github1](https://github.com/liliu-avril/Awesome-Segment-Anything)] [[Github2](https://github.com/Vision-Intelligence-and-Robots-Group/Awesome-Segment-Anything)]

- X-AnyLabeling [[Github](https://github.com/CVHub520/X-AnyLabeling)]

---

## General
- [Arxiv'23] Can SAM Segment Anything? When SAM Meets Camouflaged Object Detection [[paper](https://arxiv.org/abs/2304.04709)] [[code](https://github.com/luckybird1994/SAMCOD)]

- [NeurIPS'23] Weakly-supervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping [[paper](https://arxiv.org/abs/2305.11003)]

- [Arxiv'23] Faster Segment Anything: Towards Lightweight SAM for Mobile Applications [[paper](https://arxiv.org/abs/2306.14289)] [[code](https://github.com/ChaoningZhang/MobileSAM)]

- **Highlight** [Arxiv'23] Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection [[paper](https://arxiv.org/abs/2303.05499)] [[code](https://github.com/IDEA-Research/GroundingDINO)]
    - [Arxiv'24] Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks [[paper](https://arxiv.org/abs/2401.14159)] [[code](https://github.com/IDEA-Research/Grounded-Segment-Anything)]
    - [Arxiv'24] Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection [[paper](https://arxiv.org/abs/2405.10300)] [[code](https://github.com/IDEA-Research/Grounding-DINO-1.5-API)]

- [Arxiv'24] PosSAM: Panoptic Open-vocabulary Segment Anything [[paper](https://arxiv.org/abs/2403.09620)] [[code](https://github.com/Vibashan/PosSAM)] [[blog](https://mp.weixin.qq.com/s/GnZbPy-gVVW-VkZbS5XecA)]

- [Arxiv'24] SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash Attention to Achieve 30 times Acceleration [[paper](https://arxiv.org/abs/2403.09195)] [[blog](https://mp.weixin.qq.com/s/EblSoIhWzLHAlqyQgcN18Q)]

- [Arxiv'24] **SAM+多模态大模型** LaSagnA: Language-based Segmentation Assistant for Complex Queries [[paper](https://arxiv.org/abs/2404.08506)] [[code](https://github.com/congvvc/LaSagnA)] [[blog](https://mp.weixin.qq.com/s/2PwMZ7TmW0Rz-wACLkCD-A)]

- [ICASSP'24] Segment Anything Model Meets Image Harmonization [[paper](https://arxiv.org/abs/2312.12729)] [[blog](https://mp.weixin.qq.com/s/iUYob1ABzyVytLPtxLW9Vw)]

- [Sensors'24] **SAM+Point** FusionVision: A comprehensive approach of 3D object reconstruction and segmentation from RGB-D cameras using YOLO and fast segment anything [[paper](https://arxiv.org/abs/2403.00175)] [[code](https://github.com/safouaneelg/FusionVision/)] [[blog](https://mp.weixin.qq.com/s/IHOBJb4EKicXJQi-UGFmWA)]

- [AAAI'24] SAM-PARSER: Fine-tuning SAM Efficiently by Parameter Space Reconstruction [[paper](https://arxiv.org/abs/2308.14604)]

- [ICLR'24] Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model [[paper](https://arxiv.org/abs/2401.17868)] [[code](https://github.com/autogluon/autogluon)] [[blog](https://mp.weixin.qq.com/s/koTS9CWFQCoO7MXNYV2Dfw)]

- [ICML'24] BLO-SAM: Bi-level Optimization Based Finetuning of the Segment Anything Model for Overfitting-Preventing Semantic Segmentation [[paper](https://openreview.net/forum?id=qRtM5EqE9l)] [[code](https://github.com/importZL/BLO-SAM)] [[blog](https://mp.weixin.qq.com/s/xYBOoJjWDKgYgIth8BW8Cg)]

- [AAAI'24] Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects [[paper](https://arxiv.org/abs/2312.07374)] [[code](https://github.com/jyLin8100/GenSAM)] [[blog](https://mp.weixin.qq.com/s/UFB9THQ1zhGuvrZSMN93mg)] **GenSAM**

- [CVPR'24] RobustSAM: Segment Anything Robustly on Degraded Images [[paper](https://arxiv.org/abs/2406.09627)] [[code](https://github.com/robustsam/RobustSAM)] [[blog](https://mp.weixin.qq.com/s/a2w_GT1Bi151_rvrtuie2g)]

- [CVPR'24] Matching Anything by Segmenting Anything [[paper](https://arxiv.org/abs/2406.04221)] [[code](https://github.com/siyuanliii/masa)] [[blog](https://mp.weixin.qq.com/s/cZsBn_FOlGFijFBKocaRyQ)]

- [CVPR'24] Moving Object Segmentation: All You Need Is SAM (and Flow) [[paper](https://arxiv.org/abs/2404.12389)] [[code](https://github.com/Jyxarthur/flowsam)] [[blog](https://mp.weixin.qq.com/s/AEc8YZzD6uapTqqLe7GXzA)] [[blog2](https://mp.weixin.qq.com/s/DGq_iaBe5YmHyTu7JuHgtA)]

- [CVPR'24] Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation [[paper](https://arxiv.org/abs/2312.03502)] [[code](https://github.com/Zhang-Haojie/WeSAM)] [[blog](https://mp.weixin.qq.com/s/KzQTWVGPuSScy_p2J6kqIQ)] **WeSAM**

- [CVPR'24] SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding [[paper](https://arxiv.org/abs/2310.15308)]

- [CVPR'24] SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation [[paper](https://arxiv.org/abs/2311.15707)] [[code](https://github.com/JiehongLin/SAM-6D)] [[blog](https://mp.weixin.qq.com/s/AmBymiKnL6wIbTd1MZDOYg)]

- [CVPR'24] VRP-SAM: SAM with Visual Reference Prompt [[paper](https://arxiv.org/abs/2402.17726)] [[code](https://github.com/syp2ysy/VRP-SAM)]

- [CVPR'24] PTQ4SAM: Post-Training Quantization for Segment Anything [[paper](https://arxiv.org/abs/2405.03144)] [[code](https://github.com/chengtao-lv/PTQ4SAM)] [[blog](https://mp.weixin.qq.com/s/wK9o4ul_Z_NqGzd7shCOAg)]

- [CVPR'24] ASAM: Boosting Segment Anything Model with Adversarial Tuning [[paper](https://arxiv.org/abs/2405.00256)] [[code](https://github.com/luckybird1994/ASAM)] [[blog](https://mp.weixin.qq.com/s/syBKVIiblz7WVMO7EYMiKw)]

- [CVPR'24] BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model [[paper](https://arxiv.org/abs/2401.02317)] [[code](https://github.com/zongzi13545329/BA-SAM)]

- [CVPR'24] Endow SAM with Keen Eyes: Temporal-spatial Prompt Learning for Video Camouflaged Object Detection [[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Hui_Endow_SAM_with_Keen_Eyes_Temporal-spatial_Prompt_Learning_for_Video_CVPR_2024_paper.html)]

- [CVPR'24] EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything [[paper](https://arxiv.org/abs/2312.00863)] [[code](https://github.com/yformer/EfficientSAM)] [[project](https://yformer.github.io/efficient-sam/)] [[blog](https://mp.weixin.qq.com/s/A1G_3tNSFx0GyK2qp3iIvg)] [[blog2](https://mp.weixin.qq.com/s/AHYcJHAklHBVsjgBOXU80Q)]

- [CVPRW'24] EfficientViT-SAM: Accelerated Segment Anything Model Without Accuracy Loss [[paper](https://arxiv.org/abs/2402.05008)] [[code](https://github.com/mit-han-lab/efficientvit)] [[project](https://evitsam.hanlab.ai/)] [[blog](https://mp.weixin.qq.com/s/CH4hQTj8pEVEsx5QEwpcIg)]

- [ECCV'24] Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively [[paper](https://arxiv.org/abs/2401.02955)] [[code](https://github.com/HarborYuan/ovsam)] [[project](https://www.mmlab-ntu.com/project/ovsam/)] [[blog](https://mp.weixin.qq.com/s/wzrfQQFCLbbSED85GdXcXg)] [[blog2](https://mp.weixin.qq.com/s/BP0nEVuONODOqXkhE2gKIw)]

- [Arxiv'24] HRSAM: Efficiently Segment Anything in High-Resolution Images [[paper](https://arxiv.org/abs/2407.02109)] [[code](https://github.com/YouHuang67/High-Resolution-Segment-Anything)] [[blog](https://mp.weixin.qq.com/s/51MzrfPJTBpgWrYQ8PgUbQ)]

- [Arxiv'24] Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment Anything Model [[paper](https://arxiv.org/abs/2406.19369)] [[code](https://github.com/HarborYuan/ovsam)] **Open-Vocabulary SAM**

- [Arxiv'24] Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation [[paper](https://arxiv.org/abs/2408.15876)] [[code](https://github.com/appletea233/AL-Ref-SAM2)] [[blog](https://mp.weixin.qq.com/s/uLvW3swjVwA8MlCcPsZBwA)] **GPT-4 + SAM2**

- [Arxiv'24] Measure Anything: Real-time, Multi-stage Vision-based Dimensional Measurement using Segment Anything [[paper](https://arxiv.org/abs/2412.03472)]

---

## Medical
- [MIA'23] Segment anything model for medical image analysis: An experimental study [[paper](https://arxiv.org/abs/2304.10517)] [[code](https://github.com/mazurowski-lab/segment-anything-medical-evaluation)] [[blog](https://mp.weixin.qq.com/s/3a1mxyx9dG9lGzqP2V9foQ)]

- [Arxiv'23] Can SAM Segment Polyps? [[paper](https://arxiv.org/abs/2304.07583)] [[code](https://github.com/taozh2017/Awesome-Polyp-Segmentation)]

- [MICCAI'23] SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation [[paper](https://arxiv.org/abs/2308.07156)]

- [ICCV'23] Sam-adapter: Adapting segment anything in underperformed scenes [[paper](https://openaccess.thecvf.com/content/ICCV2023W/VCL/html/Chen_SAM-Adapter_Adapting_Segment_Anything_in_Underperformed_Scenes_ICCVW_2023_paper.html)] [[code](https://github.com/tianrun-chen/SAM-Adapter-PyTorch)]
    - [Arxiv'23] SAM Fails to Segment Anything? -- SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More [[paper](https://arxiv.org/abs/2304.09148)]

- [ICCV'23] Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models [[paper](https://arxiv.org/abs/2310.12995)]

- [Arxiv'23] SAM on Medical Images: A Comprehensive Study on Three Prompt Modes [[paper](https://arxiv.org/abs/2305.00035)]

- [Arxiv'23] Ladder Fine-tuning approach for SAM integrating complementary network [[paper](https://arxiv.org/abs/2306.12737)] [[code](https://github.com/11yxk/SAM-LST)]

- [Arxiv'23] 3DSAM-adapter: Holistic Adaptation of SAM from 2D to 3D for Promptable Medical Image Segmentation [[paper](https://arxiv.org/abs/2306.13465)] [[code](https://github.com/med-air/3dsam-adapter)]

- [Arxiv'23] MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation [[paper](https://arxiv.org/abs/2309.08842)] [[code](https://github.com/cchen-cc/MA-SAM)]

- [Arxiv'23] SAM-Med2D [[paper](https://arxiv.org/abs/2308.16184)] [[code](https://github.com/OpenGVLab/SAM-Med2D)]

- [Arxiv'23] SAM-Med3D [[paper](https://arxiv.org/abs/2310.15161)] [[code](https://github.com/uni-medical/SAM-Med3D)]

- [Arxiv'23] SAMMed: A medical image annotation framework based on large vision model [[paper](https://arxiv.org/abs/2307.05617)]

- [Arxiv'23] Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation [[paper](https://arxiv.org/abs/2304.12620)] [[code](https://github.com/KidsWithTokens/Medical-SAM-Adapter)]

- [Arxiv'24] UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images [[paper](https://arxiv.org/abs/2402.16663)] [[code](https://github.com/CUHK-AIM-Group/UN-SAM)]

- [NC'24] Segment anything in medical images [[paper](https://www.nature.com/articles/s41467-024-44824-z)] [[code](https://github.com/bowang-lab/MedSAM)]

- [MIA'24] Segment Anything Model for Medical Images? [[paper](https://arxiv.org/abs/2304.14660)] [[code](https://github.com/yuhoo0302/segment-anything-model-for-medical-images)]

- [CVPR'24] Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding [[paper](https://arxiv.org/abs/2403.18271)] [[code](https://github.com/Cccccczh404/H-SAM)]

- [MICAAI'24] Beyond Adapting SAM: Towards End-to-End Ultrasound Image Segmentation via Auto Prompting [[paper](https://arxiv.org/abs/2309.06824)] [[code](https://github.com/xianlin7/SAMUS)] **SAMUS**

- [Arxiv'24] nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance [[paper](https://arxiv.org/abs/2309.16967)] [[code](https://github.com/kent0n-li/medical-image-segmentation)]

- [Arxiv'24] ProtoSAM -- One Shot Medical Image Segmentation With Foundational Models [[paper](https://arxiv.org/abs/2407.07042)] [[code](https://github.com/Cccccczh404/H-SAM)]

---

## Remote Sensing
- [JAG'23] The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot [[paper](https://arxiv.org/abs/2306.16623)] [[code](https://github.com/opengeos/segment-geospatial)]

- [TGARS'24] RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model [[paper](https://arxiv.org/abs/2306.16269)] [[code](https://github.com/KyanChen/RSPrompter)] [[HugFace](https://huggingface.co/spaces/KyanChen/RSPrompter)] [[blog](https://mp.weixin.qq.com/s/A8lOQ33cwQSUl_vURH1oJQ)]

- [CVPRW'24] Segment Anything Model for Road Network Graph Extraction [[paper](http://arxiv.org/abs/2403.16051)] [[code](https://github.com/htcr/sam_road)] [[blog](https://mp.weixin.qq.com/s/iQS_xwPmIFAFDHgM3Wwbuw)]

---

## 3-D
- [Arxiv'24] SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners [[paper](https://arxiv.org/abs/2408.16768)] [[code](https://github.com/ZiyuGuo99/SAM2Point)] [[blog](https://mp.weixin.qq.com/s/cdSpiZu-vechAD53f6od9A)]

---

## Others
- [MM'24] Multi-scale and detail-enhanced segment anything model for salient object detection [[paper](https://arxiv.org/abs/2408.04326)] [[code](https://github.com/BellyBeauty/MDSAM)] [[blog](https://mp.weixin.qq.com/s/K9tgaNlVjy6QKFKi7C0Nww)]

---

## Blog
- [从零解读SAM(Segment Anything Model)大模型！万物皆可分割！(含源码解析)](https://mp.weixin.qq.com/s/KkCPKCzjjkXZJjZL_tznfA)

- [基于YOLO-World+EfficientSAM的零样本目标检测与实例分割Demo](https://mp.weixin.qq.com/s/u4QBbOeNR48aF9YHWdCQsw)

- [AI标注神器 X-AnyLabeling-v2.3.0 发布！支持YOLOv8旋转目标检测、EdgeSAM、RTMO等热门模型！](https://mp.weixin.qq.com/s/Gu4lfyjcowCLBgk99Rs01w)

- [X-AnyLabeling：新一代自动标注工具，开启计算机视觉新时代](https://mp.weixin.qq.com/s/V50Uv-XmmylJzibZ3_DHAg)

- [视觉大模型：SAM（Segment Anything）及示例](https://mp.weixin.qq.com/s/vNhuXUcJXPuG5Zs0JJN-Sg)